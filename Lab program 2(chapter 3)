import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque, namedtuple
class IntersectionEnv:
    """
    Very small discrete-time 4-way intersection simulator.
    State vector (float):
      [ego_dir_onehot(4), ego_dist_to_intersection (1),
       light_state_for_each_dir (3*4 = 12 one-hot: G/Y/R each),
       num_other_in_intersection (1)]
    Action: 0=WAIT, 1=GO
    """
    DIRS = ['N','E','S','W']
    LIGHTS = ['G','Y','R']
    def __init__(self, seed=None, max_steps=50):
        self.rng = np.random.RandomState(seed)
        self.max_steps = max_steps
        self.reset()
    def reset(self):
        self.ego_dir = self.rng.randint(0,4)
        self.ego_dist = self.rng.randint(1,4)  # discrete distance
        self.light_phases = [self.rng.randint(0,3) for _ in range(4)
        self.others = []
        self.ego_in_intersection = 0
        self.steps = 0
        return self._get_state()
def _get_state(self):
        dir_onehot = np.zeros(4, dtype=np.float32)
        dir_onehot[self.ego_dir] = 1.0
        dist = np.array([self.ego_dist / 3.0], dtype=np.float32
        lights = np.zeros(12, dtype=np.float32)
        for i, p in enumerate(self.light_phases):
            lights[i*3 + p] = 1.0
        other_count = np.array([float(sum(1 for o in self.others if o[1] <= 0))], dtype=np.float32)
        st = np.concatenate([dir_onehot, dist, lights, other_count])
        return st
    def _step_lights(self):
        self.light_phases = [(p + 1) % 3 for p in self.light_phases]
    def _spawn_others(self):
        for d in range(4):
            if self.rng.rand() < 0.08:
                self.others.append([d, self.rng.randint(1,4)])
    def _advance_others(self):
        new = []
        for dir_, dist in self.others:
            dist -= 1
            if dist >= -1:  # keep until they've passed
                new.append([dir_, dist])
        self.others = new
    def step(self, action):
        """
        action: 0 wait, 1 go (attempt to enter intersection if near)
        returns: state, reward, done, info
        """
        self.steps += 1
        reward = 0.0
        done = False
        info = {}
        reward -= 1.0
        self._advance_others()
        self._spawn_others()
        if self.ego_in_intersection == 0:
            if action == 1 and self.ego_dist <= 1
                phase = self.light_phases[self.ego_dir]
                if phase == 0:  
                    self.ego_in_intersection = 1
                elif phase == 1: 
                    if random.random() < 0.5:
                        self.ego_in_intersection = 1
                    else:
                        pass
                else:
                    pass
            else:
                if action == 0:
                    pass
                else:
                    self.ego_dist = max(0, self.ego_dist - 1)
         else:
            colliders = [o for o in self.others if o[1] <= 0]
            if len(colliders) > 0:
                # collision occurs
                reward -= 10.0
                done = True
                info['collision'] = True
                return self._get_state(), reward, done, info
            else:
                reward += 5.0
                done = True
                info['success'] = True
                return self._get_state(), reward, done, info
        self._step_lights()
        if self.ego_in_intersection == 0 and action == 1 and self.ego_dist > 0:
            self.ego_dist = max(0, self.ego_dist - 1)
        if self.steps >= self.max_steps:
            done = True
            info['timeout'] = True
return self._get_state(), reward, done, info
class ActorCritic(nn.Module):
    def __init__(self, input_dim, hidden=64, n_actions=2):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
        )
        self.policy = nn.Linear(hidden, n_actions)
        self.value = nn.Linear(hidden, 1)
    def forward(self, x):
        x = self.fc(x)
        logits = self.policy(x)
        value = self.value(x)
        return logits, value.squeeze(-1)
def train_a2c(episodes=1000, gamma=0.99, lr=1e-3, rollout_len=5, seed=0):
    env = IntersectionEnv(seed=seed)
    obs_dim = env._get_state().shape[0]
    device = torch.device("cpu")
    net = ActorCritic(obs_dim).to(device)
    optimizer = optim.Adam(net.parameters(), lr=lr)
    Episode = namedtuple("Episode", ["states","actions","rewards","dones","values","logps"])
    stats = {'ep_rewards': deque(maxlen=100), 'collision':0, 'success':0}
    for ep in range(1, episodes+1):
        state = env.reset()
        ep_reward = 0.0
        done = False
        states = []
        actions = []
        rewards = []
        values = []
        logps = []
        dones = []
       while not done:
            for t in range(rollout_len):
                s_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
                logits, value = net(s_t)
                probs = torch.softmax(logits, dim=-1)
                m = torch.distributions.Categorical(probs)
                a = m.sample().item()
                logp = m.log_prob(torch.tensor(a))
                next_state, r, done, info = env.step(a)
                states.append(state)
                actions.append(a)
                rewards.append(r)
                values.append(value.item())
                logps.append(logp)
                dones.append(done)
                state = next_state
                ep_reward += r
                if done:
                    if 'collision' in info:
                        stats['collision'] += 1
                    if 'success' in info:
                        stats['success'] += 1
                    break
            if done:
                R = 0.0
            else:
                with torch.no_grad():
                    s_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
                    _, v = net(s_t)
                    R = v.item()
            returns = []
            for r in reversed(rewards):
                R = r + gamma * R
                returns.insert(0, R)
            returns = torch.tensor(returns, dtype=torch.float32)
            values_t = torch.tensor(values, dtype=torch.float32)
            logps_t = torch.stack(logps)
            advs = returns - values_t
            value_loss = 0.5 * (advs ** 2).mean()
            policy_loss = -(logps_t * advs.detach()).mean()
            entropy = - (torch.softmax(torch.stack([net(torch.tensor(s,dtype=torch.float32).unsqueeze(0))[0].squeeze(0) for s in states]),dim=-1) * 
                        torch.log_softmax(torch.stack([net(torch.tensor(s,dtype=torch.float32).unsqueeze(0))[0].squeeze(0) for s in states]),dim=-1)).sum(dim=-1).mean()
            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(net.parameters(), 0.5)
            optimizer.step()
            states, actions, rewards, values, logps, dones = [], [], [], [], [], []
stats['ep_rewards'].append(ep_reward)
     if ep % 50 == 0:
            avg = np.mean(stats['ep_rewards']) if stats['ep_rewards'] else 0.0
            print(f"Episode {ep:4d} | AvgReward(100) {avg:6.2f} | Collisions {stats['collision']} | Success {stats['success']}")
    return net
if __name__ == "__main__":
    net = train_a2c(episodes=600, seed=123)
    print("Training finished.")

Output:
Episode   50 | AvgReward(100)  -7.46 | Collisions 23 | Success 27
Episode  100 | AvgReward(100)  -6.45 | Collisions 41 | Success 59
Episode  150 | AvgReward(100)  -5.53 | Collisions 60 | Success 90
Episode  200 | AvgReward(100)  -4.62 | Collisions 76 | Success 124
Episode  250 | AvgReward(100)  -2.92 | Collisions 89 | Success 161
Episode  300 | AvgReward(100)  -2.64 | Collisions 105 | Success 195
Episode  350 | AvgReward(100)  -2.68 | Collisions 118 | Success 232
Episode  400 | AvgReward(100)  -2.00 | Collisions 129 | Success 271
Episode  450 | AvgReward(100)  -1.90 | Collisions 142 | Success 308
Episode  500 | AvgReward(100)  -2.14 | Collisions 155 | Success 345
Episode  550 | AvgReward(100)  -1.78 | Collisions 164 | Success 386
Episode  600 | AvgReward(100)  -2.85 | Collisions 184 | Success 416
Training finished.

